# Copyright 2018 The GraphNets Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or  implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ============================================================================
"""Model architectures for the demos."""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

from graph_nets import modules
from graph_nets import utils_tf
from graph_nets import blocks
import sonnet as snt
import tensorflow as tf

import os
import pickle

# from IfNet.models import simple_desc
# from IfNet.common.argparse_utils import *

import GM_GenData

NUM_LAYERS = 2  # Hard-code number of layers in the edge/node/global models.
if GM_GenData.DATASET=="Willow":
  LATENT_SIZE = GM_GenData.LATENT_DIM_WILLOW#16 #64 # Hard-code latent layer sizes for demos.
elif GM_GenData.DATASET=="Pascal":
  LATENT_SIZE = GM_GenData.LATENT_DIM_PASCAL
elif GM_GenData.DATASET=="Spair71k":
  LATENT_SIZE = GM_GenData.LATENT_DIM_SPAIR71K
elif GM_GenData.DATASET == "QAPDATA":
  LATENT_SIZE = GM_GenData.LATENT_DIM_QAPDATA
elif GM_GenData.DATASET == "CUB_2011":
  LATENT_SIZE = GM_GenData.LATENT_DIM_CUB
elif GM_GenData.DATASET == "ICMPT":
  LATENT_SIZE = GM_GenData.LATENT_DIM_ICMPT

# LATENT_SIZE = 32
def make_mlp_model():
  """Instantiates a new MLP, followed by LayerNorm.

  The parameters of each new MLP are not shared with others generated by
  this function.

  Returns:
    A Sonnet module which contains the MLP and LayerNorm.
  """
  return snt.Sequential([
      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=True),
      # GlobalNorm()
      snt.LayerNorm(),
      # lambda x: tf.nn.dropout(x, 0.5)
  ])

class GlobalNorm(snt.AbstractModule):
  def __init__(self, name = "GlobalNorm"):
    super(GlobalNorm, self).__init__(name= name)

  def _build(self, input):
    mean_ = tf.math.reduce_mean(input, axis=0)
    var_ = tf.math.reduce_variance(input, axis=0)

    return (input - mean_) / tf.math.sqrt(var_ + 0.0001)

class MLPGraphIndependent(snt.AbstractModule):
  """GraphIndependent with MLP edge, node, and global models."""

  def __init__(self, name="MLPGraphIndependent"):
    super(MLPGraphIndependent, self).__init__(name=name)
    with self._enter_variable_scope():
      self._network = modules.GraphIndependent(
          edge_model_fn=make_mlp_model,
          node_model_fn=make_mlp_model,
       #   group_model_fn=make_mlp_model,
          global_model_fn=make_mlp_model)

  def _build(self, inputs,keepprob):
    output=self._network(inputs,keepprob)

    return output


class MLPGraphNetwork(snt.AbstractModule):
  """GraphNetwork with MLP edge, node, and global models."""

  def __init__(self, name="MLPGraphNetwork"):
    super(MLPGraphNetwork, self).__init__(name=name)
    with self._enter_variable_scope():
      self._network = modules.GraphNetwork(make_mlp_model, make_mlp_model,make_mlp_model,
                                           make_mlp_model)

  def _build(self, inputs,keep_prob):
    return self._network(inputs,keep_prob)


class AffinityNetwork(snt.AbstractModule):
  """GraphNetwork with Edge affinity learning."""

  def __init__(self,
               node_input_size,
               name="AffinityNetwork"):
    super(AffinityNetwork, self).__init__(name=name)

    with self._enter_variable_scope():
      self._metric1 = tf.Variable(
        tf.random_normal(shape=[node_input_size, node_input_size], dtype = tf.float64, mean=0, stddev=1),
        dtype = tf.float64,
        name='affinity_metric1')
      self._metric2 = tf.Variable(
        tf.random_normal(shape=[node_input_size, node_input_size], dtype = tf.float64, mean=0, stddev=1),
        dtype = tf.float64,
        name='affinity_metric2')

  def _build(self, graph):
    sender_feature = blocks.broadcast_sender_nodes_to_edges(graph)
    receiver_feature = blocks.broadcast_receiver_nodes_to_edges(graph)
    correlation = tf.multiply(tf.matmul(sender_feature, self._metric1), tf.matmul(receiver_feature, self._metric2))
    edges = tf.nn.relu(tf.reshape(tf.reduce_sum(correlation, axis = 1), shape = (-1, 1)))
    return graph.replace(edges = edges)

class NodeFeaNetwork(snt.AbstractModule):
  """GraphNetwork with Node feature learning."""

  def __init__(self,
               is_training,
               name="NodeFeaNetwork"):
    super(NodeFeaNetwork, self).__init__(name=name)

    parser = get_parser()

    general_arg = add_argument_group('General', parser)
    general_arg.add_argument('--num_threads', type=int, default=8,
                            help='the number of threads (for dataset)')

    io_arg = add_argument_group('In/Out', parser)
    io_arg.add_argument('--in_dir', type=str, default='./samples',
                            help='input image directory')
    # io_arg.add_argument('--in_dir', type=str, default='./release/outdoor_examples/images/sacre_coeur/dense/images',
    #                         help='input image directory')
    io_arg.add_argument('--out_dir', type=str, default='./dump_feats',
                            help='where to save keypoints')
    io_arg.add_argument('--full_output', type=str2bool, default=True,
                            help='dump keypoint image')

    model_arg = add_argument_group('Model', parser)
    model_arg.add_argument('--model', type=str, default='./IfNet/release/models/outdoor/',
                            help='model file or directory')
    model_arg.add_argument('--top_k', type=int, default=500,
                            help='number of keypoints')
    model_arg.add_argument('--max_longer_edge', type=int, default=-1,
                            help='resize image (do nothing if max_longer_edge <= 0)')

    tmp_config, unparsed = get_config(parser)

    if len(unparsed) > 0:
        raise ValueError('Miss finding argument: unparsed={}\n'.format(unparsed))

    # restore other hyperparams to build model
    if os.path.isdir(tmp_config.model):
        config_path = os.path.join(tmp_config.model, 'config.pkl')
    else:
        config_path = os.path.join(os.path.dirname(tmp_config.model), 'config.pkl')

    try:
        with open(config_path, 'rb') as f:
            config = pickle.load(f)
        #    print_opt(config)
    except:
        raise ValueError('Fail to open {}'.format(config_path))

    for attr, dst_val in sorted(vars(tmp_config).items()):
        if hasattr(config, attr):
            src_val = getattr(config, attr)
            if src_val != dst_val:
                setattr(config, attr, dst_val)
        else:
            setattr(config, attr, dst_val)

    self._descriptor = simple_desc.Model(config, is_training = is_training)

  def _build(self, graph):
    patches1 = tf.expand_dims(graph.patches1, -1)
    patches2 = tf.expand_dims(graph.patches2, -1)
    fea1, _ = self._descriptor.build_model(patches1, reuse=False)
    fea2, _ = self._descriptor.build_model(patches2, reuse=True)

    assign_fea1 = tf.gather(fea1, graph.group_indices_1)
    assign_fea2 = tf.gather(fea2, graph.group_indices_2)
    node_feas = tf.cast(tf.concat([assign_fea1, assign_fea2], 1), dtype = tf.float64)

#    node_feas = tf.cast(tf.concat([fea1, fea2], 1), dtype = tf.float64)

    return graph.replace(nodes = node_feas)

class EncodeProcessDecode(snt.AbstractModule):
  """Full encode-process-decode model.

  The model we explore includes three components:
  - An "Encoder" graph net, which independently encodes the edge, node, and
    global attributes (does not compute relations etc.).
  - A "Core" graph net, which performs N rounds of processing (message-passing)
    steps. The input to the Core is the concatenation of the Encoder's output
    and the previous output of the Core (labeled "Hidden(t)" below, where "t" is
    the processing step).
  - A "Decoder" graph net, which independently decodes the edge, node, and
    global attributes (does not compute relations etc.), on each message-passing
    step.

                      Hidden(t)   Hidden(t+1)
                         |            ^
            *---------*  |  *------*  |  *---------*
            |         |  |  |      |  |  |         |
  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)
            |         |---->|      |     |         |
            *---------*     *------*     *---------*
  """

  def __init__(self,
               visfeaType = None,
               dynamic_core_num = 0,
               edge_output_size=None,
               node_output_size=None,
               group_output_size=None,
               global_output_size=None,
               name="EncodeProcessDecode"):
    super(EncodeProcessDecode, self).__init__(name=name)

    if visfeaType == GM_GenData.VISFEA_TYPE_LFNET:
      self._node_descriptor = NodeFeaNetwork(is_training = True)
    else:
      self._node_descriptor = None

    self._encoder = MLPGraphIndependent()
#    self._core = MLPGraphNetwork()
    self._decoder = MLPGraphIndependent()

    self._dynamic_core_num = dynamic_core_num
    if self._dynamic_core_num > 0:
      self._dynamic_cores = []
      for _ in range(self._dynamic_core_num):
        self._dynamic_cores.append(MLPGraphNetwork())
    else:
      self._core = MLPGraphNetwork()

    # Transforms the outputs into the appropriate shapes.
    if edge_output_size is None:
      edge_fn = None
    else:
      edge_fn = lambda: snt.Linear(edge_output_size, name="edge_output")

    if node_output_size is None:
      node_fn = None
    else:
      node_fn = lambda: snt.Linear(node_output_size, name="node_output")

    if group_output_size is None:
      group_fn = None
    else:
      group_fn = lambda: snt.Linear(group_output_size, name="group_output")

    if global_output_size is None:
      global_fn = None
    else:
      global_fn = lambda: snt.Linear(global_output_size, name="global_output")

    with self._enter_variable_scope():
    #  self._output_transform = modules.GraphIndependent(edge_fn, node_fn, global_fn)
      self._output_transform = modules.GraphIndependentReLU(edge_fn, node_fn, global_fn)
      self._group_aggregator = modules.GraphGroupAggregator(group_fn)

  def gen_affi_tensor(self,graphs):
    nums_match=graphs.n_node
    nums_edge=graphs.n_edge
    batch=len(nums_match)
    receivers,senders=graphs.receivers,graphs.senders
    max_match=tf.maximum(nums_match)
    Ks=tf.ones((batch,max_match,max_match))

    num_head=0
    for idx in range(batch):
      num_tail=num_head+nums_edge[idx]
      Ks[idx,receivers[num_head:num_tail],senders[num_head:num_tail]]=graphs.edges[num_head:num_tail]
      num_head=num_tail

    return Ks

  def _prob_matching(self,graphs,batch):

    def return_fun(ret):
      return ret
    iteration=100
    sum_kpts = tf.cast(tf.reduce_sum(graphs.n_node), tf.int64)
    index_K=tf.concat((tf.expand_dims(graphs.receivers,axis=1),
                       tf.expand_dims(graphs.senders,axis=1)),axis=1)
    index_K=tf.cast(index_K,dtype=tf.int64)
    size_K=tf.cast(tf.reduce_sum(graphs.n_node),tf.int64)
    size_K_f=tf.cast(tf.reduce_sum(graphs.n_node),tf.float64)
    value_K=tf.squeeze(graphs.edges,axis=1)
    Ks=tf.SparseTensor(indices=index_K, values=value_K, dense_shape=[size_K, size_K])
    Ps = graphs.nodes
    # Ps=tf.ones((size_K,1),dtype=tf.float64)

    for ite in range(iteration):
      Ps_last = Ps
      Qs = tf.sparse.sparse_dense_matmul(Ks, Ps)

      if ite % 2 == 1:
        groups_1 = tf.unsorted_segment_sum(Qs, graphs.group_indices_1, sum_kpts)
        normalize_1 = tf.gather(groups_1, graphs.group_indices_1)
        Ps = Qs / (normalize_1 + 1e-8)
      else:
        groups_2 = tf.unsorted_segment_sum(Qs, graphs.group_indices_2, sum_kpts)
        normalize_2 = tf.gather(groups_2, graphs.group_indices_2)
        Ps = Qs / (normalize_2 + 1e-8)

      delta= Ps / (Ps_last + 10e-6)
      vector_ones=tf.ones((1,size_K),dtype=tf.float64)
      Ks = Ks * (tf.matmul(delta,vector_ones))
      # Ps = tf.cond(tf.norm(Ps-Ps_last)>0.000001, lambda : return_fun(Ps), lambda : return_fun(Ps_last))

      # if tf.less(tf.norm(Ps-Ps_last),0.000001):
      #   return Ps

    return Ps

  def _build(self, input_op, num_processing_steps,keepprob_encoder,keepprob_decoder,keepprob_conv):
    if self._node_descriptor is not None:
      input_op = self._node_descriptor(input_op)

    latent = self._encoder(input_op,keepprob_encoder)
    # latent = tf.nn.dropout(latent,keep_prob=0.7)
    #latent = input_op
    latent0 = latent
    output_ops = []
    output_ops_trasform=[]
    for i in range(num_processing_steps):
      core_input = utils_tf.concat([latent0, latent], axis=1)
      if self._dynamic_core_num > 0:
        latent = self._dynamic_cores[i](core_input)
      else:
        latent = self._core(core_input,keepprob_conv)

    decoded_op = self._decoder(latent,keepprob_decoder)
    # decoded_op =tf.nn.dropout(decoded_op,keep_prob=0.7)
    decoded_op_prev = decoded_op
    # output_ops.append(self._output_transform(decoded_op))
    # output_ops_trasform.append(self._output_transform(decoded_op))
    output_ops_trasform=self._output_transform(decoded_op)
    # match_tensor=self._prob_matching(output_ops_trasform,3)
    # decoded_op_prev=output_ops_trasform
    #
    # output_ops_trasform.replace(nodes=match_tensor)
    # decoded_op_lst=output_ops_trasform
    # output_ops.append(self._group_aggregator(self._output_transform(decoded_op)))
    output_ops.append(self._group_aggregator(output_ops_trasform))
    return output_ops
